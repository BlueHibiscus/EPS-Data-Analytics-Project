{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74b60c62",
   "metadata": {
    "id": "74b60c62"
   },
   "source": [
    "## Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f04d49",
   "metadata": {
    "id": "61f04d49"
   },
   "source": [
    "# Tasks:\n",
    "1. Build a EPS database which stores the analysts forecasts and real EPS information for 29 companies \n",
    "2. Scraping the data to retrieve the following in the form of tables from the Estimize.com for all available quarters in 2022, 2021, and 2020.\n",
    "            - Company Basic Information\n",
    "            - Company Forecasts\n",
    "            - Analysts Information\n",
    "            - Stocks Covered\n",
    "            - Pending Estimates\n",
    "            - Scored Estimates\n",
    "            \n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ece047",
   "metadata": {
    "id": "c6ece047"
   },
   "source": [
    "# Company Basic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc920d06",
   "metadata": {
    "id": "fc920d06"
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11791921",
   "metadata": {
    "id": "11791921"
   },
   "outputs": [],
   "source": [
    "base_url = \"https://www.estimize.com/edge\"\n",
    "other_half_url = '?metric_name=eps&chart=historical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc7e2f9",
   "metadata": {
    "id": "abc7e2f9"
   },
   "outputs": [],
   "source": [
    "Ticker_list = ['AMZN', 'AAPL', 'MSFT', 'GOOG', 'TSLA', 'JNJ', 'PG', 'NVDA', 'CSCO', 'BABA', 'HD', \n",
    "'BIDU', 'WMT', 'CRM', 'LULU', 'TGT', 'PANW', 'ADBE', 'VMW', 'MU', 'NKE', 'ORCL', 'BB', 'HPQ', 'COST', 'AMAT', \n",
    "'BAC', 'CVX', 'AMGN']\n",
    "\n",
    "\n",
    "for i in range(len(Ticker_list)):\n",
    "    Ticker_list[i] = Ticker_list[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6db81e",
   "metadata": {
    "id": "6d6db81e"
   },
   "outputs": [],
   "source": [
    "url = []\n",
    "for i in range(len(Ticker_list)):\n",
    "    \n",
    "    site_name= base_url[:25] + Ticker_list[i] +'/fq1-2020'+ other_half_url\n",
    "    url.append(site_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfaf6ab",
   "metadata": {
    "id": "5dfaf6ab"
   },
   "source": [
    "- Chrome WebDriver is installed and used here in order to run the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4dbb12",
   "metadata": {
    "id": "4b4dbb12"
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9449423f",
   "metadata": {
    "id": "9449423f"
   },
   "outputs": [],
   "source": [
    "ticker = []\n",
    "name = []\n",
    "sectors = []\n",
    "industries = []\n",
    "followers = []\n",
    "analysts = []\n",
    "for i in url:\n",
    "    driver.implicitly_wait(10)\n",
    "    driver.get(i)\n",
    "    driver.implicitly_wait(10)\n",
    "    ticker.append(driver.find_element_by_class_name(\"release-header-information-title\").text)\n",
    "    name.append(driver.find_element_by_class_name(\"release-header-information-description\").text)\n",
    "    sectors.append(driver.find_element_by_xpath('//*[@id=\"releases_show\"]/div[2]/div[2]/div/div[1]/div/div/div/p/span[1]/a/span').text)\n",
    "    industries.append(driver.find_element_by_xpath('//*[@id=\"releases_show\"]/div[2]/div[2]/div/div[1]/div/div/div/p/span[2]/a/span').text)\n",
    "    followers.append(driver.find_element_by_xpath('//*[@id=\"summary-stats\"]/div/div/div[1]/div[2]').text)\n",
    "    analysts.append(driver.find_element_by_xpath('//*[@id=\"summary-stats\"]/div/div/div[2]/a').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751179ef",
   "metadata": {},
   "source": [
    "- The data is scraped using Selenium. \n",
    "- Xpath is choosen as as unique identifier for scraping the data of all columns in Company Basic information table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92ef483",
   "metadata": {
    "id": "f92ef483"
   },
   "outputs": [],
   "source": [
    "dbms_info = pd.DataFrame({\"Ticker_list\" : ticker, \n",
    "    \"Company Name\" : name,\n",
    "    \"Sectors\" : sectors,\n",
    "    \"Industries\" : industries,\n",
    "    \"Number Of Followers\" : followers,\n",
    "    \"Number Of Analysts\" : analysts   \n",
    "   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biCUbm4WRaeG",
   "metadata": {
    "id": "biCUbm4WRaeG"
   },
   "outputs": [],
   "source": [
    "dbms_info1 = dbms_info.assign(NO_OF_FOLLOWERS = dbms_info['Number Of Followers'].str.replace(\",\",\"\"), \n",
    "                              NO_OF_ANALYSTS = dbms_info['Number Of Analysts'].str.replace(\",\",\"\"))\n",
    "dbms_info1.drop(['Number Of Followers', 'Number Of Analysts'], axis =1, inplace = True)\n",
    "dbms_info1.columns = ['TICKER', 'COMPANY_NAME', 'SECTOR', 'INDUSTRY', 'NO_OF_FOLLOWERS', 'NO_OF_ANALYSTS']\n",
    "dbms_info2 = dbms_info1.drop_duplicates('TICKER')\n",
    "dbms_info2.to_sql(\"company\", engine, index = False, if_exists = 'append')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ffc79f",
   "metadata": {},
   "source": [
    "- The Numbumber columns has , in the website. Text Processing and Data cleaning is done in order to run the table in MYSQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98719dd4",
   "metadata": {
    "id": "98719dd4"
   },
   "source": [
    "- The Above Dataframe contains the Basic Information of 29 Companies.\n",
    "- The columns include:\n",
    "            1.Ticker_list\n",
    "            2.Company Name\n",
    "            3.Sectors\n",
    "            4.Industries\n",
    "            5.Number Of Followers\n",
    "            6.Number Of Analysts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a2ea3b",
   "metadata": {
    "id": "c1597efc"
   },
   "outputs": [],
   "source": [
    "dbms_info1 = dbms_info.assign(NO_OF_FOLLOWERS = dbms_info['Number Of Followers'].str.replace(\",\",\"\"), \n",
    "                              NO_OF_ANALYSTS = dbms_info['Number Of Analysts'].str.replace(\",\",\"\"))\n",
    "dbms_info1.drop(['Number Of Followers', 'Number Of Analysts'], axis =1, inplace = True)\n",
    "dbms_info1.columns = ['TICKER', 'COMPANY_NAME', 'SECTOR', 'INDUSTRY', 'NO_OF_FOLLOWERS', 'NO_OF_ANALYSTS']\n",
    "dbms_info2 = dbms_info1.drop_duplicates('TICKER')\n",
    "dbms_info2.to_sql(\"company\", engine, index = False, if_exists = 'append')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5e76ce",
   "metadata": {},
   "source": [
    "- The above code bring the Dataframe in to MYSQL Databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d6469a",
   "metadata": {
    "id": "64d6469a"
   },
   "source": [
    "### Company Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1uC9J1-lSChr",
   "metadata": {
    "id": "1uC9J1-lSChr"
   },
   "outputs": [],
   "source": [
    "analyst_type = []\n",
    "analysts = []\n",
    "values_list = []\n",
    "hyperlinks = []\n",
    "ticker_names = []\n",
    "quarter_name = []\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "for u in range(len(url)):\n",
    "    url_i =  url[u]\n",
    "    driver.implicitly_wait(75)\n",
    "    driver.get(url_i)\n",
    "    driver.implicitly_wait(75)\n",
    "    try:\n",
    "        num_full = driver.find_element(By.CLASS_NAME, \"estimates-tbl-count\").text\n",
    "        index_value = (num_full.find('/'))\n",
    "        condition = num_full.replace(\"Showing \",\"\")\n",
    "\n",
    "        get_num = condition[0:2]\n",
    "        get_num = pd.to_numeric(get_num)\n",
    "        num = num_full[index_value+1:].replace(\"estimates\",\"\")\n",
    "        num = pd.to_numeric(num)\n",
    "        if num > 30:\n",
    "            btn = driver.find_element(By.CLASS_NAME, \"pagination-footer\")\n",
    "            btn.click()\n",
    "        driver.implicitly_wait(75)\n",
    "        \n",
    "        ticker_i = ticker_copy_list[u]\n",
    "        tdl = []\n",
    "        tdl.append(ticker_i)\n",
    "        ql = []\n",
    "        ql.append(quarter_list[u])\n",
    "\n",
    "        # Reading values in one shot\n",
    "        soup = BeautifulSoup(driver.page_source)\n",
    "        soup_table = soup.find_all(\"table\")[-1]\n",
    "        tables = pd.read_html(str(soup_table))\n",
    "        values = tables[0].drop([\"Chart\",\"Unnamed: 1\",\"Rank\",\"Points\",\"Confidence\",\"Last Revised\",\"Analyst\"],axis=1)\n",
    "        values1 = values.Value.to_list()\n",
    "        \n",
    "        links = []\n",
    "        for j in range(0,num):\n",
    "            links.append(driver.find_elements(By.XPATH,\"//a[@class = 'username']\")[j]) \n",
    "        \n",
    "        table = soup.find('tbody', attrs={'class':'estimates-tbl-consensus'})\n",
    "        table_rows  = table.find_all(\"tr\")\n",
    "        res = []\n",
    "        for tr in table_rows:\n",
    "            td = tr.find_all('td')\n",
    "            row = [tr.text.strip() for tr in td if tr.text.strip()]\n",
    "            if row:\n",
    "                res.append(row)\n",
    "                \n",
    "        result_consensus = [res[0] for res in res]\n",
    "        \n",
    "        res_con = len(result_consensus)\n",
    "        num_new = num + res_con\n",
    "        \n",
    "        ticker_names.extend(tdl*num_new)\n",
    "        quarter_name.extend(ql*num_new)\n",
    "        \n",
    "        analysts.extend(result_consensus)\n",
    "        analyst_type.extend(['OVERALL']*res_con)\n",
    "        for i in range(num):\n",
    "            analysts.append(driver.find_elements(By.CLASS_NAME,\"username\")[i].text)\n",
    "        analyst_type.extend(['INDIVIDUAL']*num)\n",
    "        values_list.extend(values1)\n",
    "        \n",
    "        hyperlinks.extend([\"NA\"]*res_con)\n",
    "        for link in links:\n",
    "            hyperlinks.append(link.get_attribute(\"href\"))\n",
    "        del tables\n",
    "        del values\n",
    "        del values1\n",
    "        driver.implicitly_wait(75)\n",
    "    except:\n",
    "        print(\"Loop Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a94a82",
   "metadata": {},
   "source": [
    "- In order to obtin all the Analys Information, Beautiful Soup and Selenium is used. \n",
    "- The values column is Extracted using Beautiful Soup, where as Selenium is used for all the columns data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oRuQ_lPjSFXz",
   "metadata": {
    "id": "oRuQ_lPjSFXz"
   },
   "outputs": [],
   "source": [
    "a_p = pd.DataFrame({\"Ticker_list\" : ticker_names, \n",
    "                       \"Quarter_year\": quarter_name,\n",
    "                       \"Analyst\" : analysts,\n",
    "                       \"Value\" : values_list,\n",
    "                       \"Analyst_type\" : analyst_type,\n",
    "                       \"URL\" : hyperlinks\n",
    "                      })\n",
    "a_p.columns = ['TICKER', 'QUARTER_YEAR', 'ANALYST', 'VALUE', 'ANALYST_TYPE', 'URL']\n",
    "a_p.to_sql(\"company_forecasts\", engine, index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9d67c6",
   "metadata": {},
   "source": [
    "- This table contains the following infomation for 29 companies for all available quarters in 2022, 2021, and 2020:\n",
    "        1. Ticker\n",
    "        2. Quarter_Year\n",
    "        3. Analyst\n",
    "        4. Value\n",
    "        5. Analyst_Type\n",
    "        6. URL\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3ddf28",
   "metadata": {
    "id": "0d3ddf28"
   },
   "source": [
    "- The Analyst_Type has 'Overall' and 'Individual' content.\n",
    "- Here Overall is assigned to Reported Earnings, Estimize Consensus, Estimize Mean and Wall Street Consensus. Individual is assigned to all the individual Analysts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5989746d",
   "metadata": {
    "id": "5989746d"
   },
   "source": [
    "### Analysts Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b02bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_urls = pd.read_csv(\"Analyst_URL.csv\")\n",
    "\n",
    "analyst_urls.drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "name = []\n",
    "role = []\n",
    "join = []\n",
    "score = []\n",
    "error = []\n",
    "accuracy = []\n",
    "points = []\n",
    "peg = []\n",
    "stocks = []\n",
    "pending = []\n",
    "url_a = []\n",
    "\n",
    "\n",
    "for url in list(analyst_urls.URL.values):\n",
    "    browser.get(url)\n",
    "    browser.implicitly_wait(5)\n",
    "    \n",
    "    name.append(browser.find_element(By.XPATH, '//*[@class=\"before-main-wrapper content-header-show\"]/div/div/div/h1/a').text)\n",
    "    \n",
    "    role.append(browser.find_element(By.XPATH, '//*[@class=\"before-main-wrapper content-header-show\"]/div/div/div/ul').text)\n",
    "    \n",
    "    b= browser.find_element(By.XPATH, '//*[@class=\"before-main-wrapper content-header-show\"]/div/div/div/div[2]').text\n",
    "    if not b or b == '-':\n",
    "        join.append('')\n",
    "    else:\n",
    "        join.append(b)\n",
    "    \n",
    "    m = browser.find_element(By.XPATH, '//*[@class=\"before-main-wrapper content-header-show\"]/div/div[2]/div/div/div[2]').text\n",
    "    if m == '-' or m == 'N/A':\n",
    "        score.append('')\n",
    "    else:\n",
    "        score.append(float(m))\n",
    "    \n",
    "    e = browser.find_element(By.XPATH, '//*[@id=\"profile-tab-wrap\"]/div/div/div').text # int and not existing\n",
    "    if e  == '-':\n",
    "        error.append('')\n",
    "    else:\n",
    "        error.append(e)\n",
    "        \n",
    "    a = browser.find_element(By.XPATH, '//*[@id=\"profile-tab-wrap\"]/div/div[2]/div').text # int and not existing\n",
    "    if a  == '-':\n",
    "        accuracy.append('')\n",
    "    else:\n",
    "        accuracy.append(a)\n",
    "        \n",
    "    p = browser.find_element(By.XPATH, '//*[@id=\"profile-tab-wrap\"]/div[2]/div/div').text # int and not existing\n",
    "    if p  == '-':\n",
    "        points.append('')\n",
    "    else:\n",
    "        points.append(p)\n",
    "        \n",
    "    pe = browser.find_element(By.XPATH, '//*[@id=\"profile-tab-wrap\"]/div[2]/div[2]/div').text # int and not existing\n",
    "    if pe  == '-':\n",
    "        peg.append('')\n",
    "    else:\n",
    "        peg.append(pe)\n",
    "        \n",
    "    s = browser.find_element(By.XPATH, '//*[@id=\"profile-tab-wrap\"]/div[3]/div/div').text # int and not existing\n",
    "    if s  == '-':\n",
    "        stocks.append('')\n",
    "    else:\n",
    "        stocks.append(s)\n",
    "        \n",
    "    pen = browser.find_element(By.XPATH, '//*[@id=\"profile-tab-wrap\"]/div[3]/div/div').text # int and not existing\n",
    "    if pen  == '-':\n",
    "        pending.append('')\n",
    "    else:\n",
    "        pending.append(pen)\n",
    "        \n",
    "    url_a.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bfd851",
   "metadata": {},
   "outputs": [],
   "source": [
    "d8 = {'Name': name, \n",
    "      'Roles' :role , \n",
    "      'Join Date':join, \n",
    "      'Analyst Confidence Score' : score, \n",
    "      'Error rate': error,\n",
    "      'Accuracy Percentile':accuracy, \n",
    "      'points' :points, \n",
    "      'points/Estimate' : \n",
    "      peg, 'stocks' : stocks, \n",
    "      'pending':pending, 'URL':url_a}  \n",
    "ana_ba_info = pd.DataFrame(data=d8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2fc23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ana_ba_info['Join1'] = ana_ba_info['Join Date'].str.replace(\".+since\\s|\\s-.+\", \"\", regex= True)\n",
    "ana_ba_info['Join2'] = ana_ba_info['Join1'].str.replace(\"Twitter|StockTwits\",\"\", regex=True)\n",
    "ana_ba_info['Join3'] = ana_ba_info['Join2'].str.replace(\"\\n\",\"\", regex=True)\n",
    "\n",
    "ana_ba_info.drop(['Join1', 'Join2', 'Join Date'], axis=1, inplace =True)\n",
    "\n",
    "ana_ba_info.columns = ['NAME', 'ROLE', 'ANALYST_CONFIDENCE_SCORE', 'ERROR_RATE', 'ACCURACY_PERCENTILE', 'POINTS', \n",
    "                       'POINTS_PER_ESTIMATE', 'STOCKS', 'PENDING', 'URL', 'JOIN_DATE']\n",
    "\n",
    "ana_ba_info1 = ana_ba_info.drop_duplicates()\n",
    "ana_ba_info1.to_sql(\"analyst_info\", engine, index = False, if_exists = 'append')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750d5c01",
   "metadata": {
    "id": "750d5c01"
   },
   "source": [
    " - This table contains the folllowing columns:\n",
    "         1. Name\n",
    "         2. Roles\n",
    "         3. Join Date\n",
    "         4. Analyst Confidence Score\n",
    "         5. Error Rate\n",
    "         6. Accuracy Percentile\n",
    "         7. Points\n",
    "         8. Points/Estimate\n",
    "         9. Stocks\n",
    "         10. Pending \n",
    "         11. URL\n",
    "         \n",
    "         \n",
    "         \n",
    "         \n",
    "- The data is scraped using Selenium. Here the XPath is choosen to scrape the data form each analyst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982a09f3",
   "metadata": {},
   "source": [
    "### Stocks Covered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc25249c",
   "metadata": {
    "id": "fc25249c"
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "path = \"C:/Users/Rida/Downloads/Stocks/\"\n",
    "lost_url = []\n",
    "for i in range(50):\n",
    "    try:\n",
    "        url_i = analysts_df.iloc[i,0]\n",
    "        driver.get(url_i)\n",
    "        driver.execute_script('window.scrollTo(0, 1200);')\n",
    "        # The above line of code is written to scroll the browser near the Stocks Covered table\n",
    "        \n",
    "        driver.implicitly_wait(50)\n",
    "\n",
    "        num = int(driver.find_elements_by_class_name('profile-tbl-pagination-total-count')[0].text)\n",
    "        if(num>10):\n",
    "            driver.implicitly_wait(50)\n",
    "            for i in range(math.ceil(num/20)):\n",
    "                btn = driver.find_element_by_xpath('//*[@id=\"profile-covered-stocks\"]/div[2]/a')\n",
    "                btn.click()\n",
    "                time.sleep(2) \n",
    "        a = driver.find_elements_by_xpath('//*[@id=\"profile-covered-stocks\"]/div[1]/div[2]')[0].text\n",
    "        \n",
    "        driver.implicitly_wait(50)\n",
    "\n",
    "        x = a.replace(\" \",\",\")\n",
    "        li = list(x.split(\"\\n\"))\n",
    "        Stocks_Covered = pd.DataFrame(li)\n",
    "        Stocks_Covered[['a', 'b','c','d','e','f','g','h']] = Stocks_Covered[0].str.split(',', expand=True)\n",
    "        Stocks_Covered[\"c\"] = Stocks_Covered['b'].astype(str) +\"-\"+ Stocks_Covered[\"c\"].astype(str)\n",
    "        Stocks_Covered.drop([0,\"b\"],axis=1,inplace=True)\n",
    "        Stocks_Covered.columns = [\"TICKER\",\"REPORTS\",\"QUARTERS\",\"POINTS\",\"PTS_EST\",\"ERROR RATE\",\"ACCURACY\"]\n",
    "        Stocks_Covered['url'] = url_i\n",
    "        path_i = path + \"Stocks_\" + str(i) +\".csv\"\n",
    "        Stocks_Covered.to_csv(path_i, index = False)\n",
    "        del Stocks_Covered\n",
    "    except:\n",
    "        lost_url.append(url_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iFnc007nSVjl",
   "metadata": {
    "id": "iFnc007nSVjl"
   },
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/Rida/Downloads/Stocks_Covered/'\n",
    "import os\n",
    "files = os.listdir(file_path)\n",
    "analyst_pred = pd.DataFrame()\n",
    "for i in range(len(files)):\n",
    "    print(i)\n",
    "    path_i = file_path + files[i]\n",
    "    temp_df = pd.read_csv(path_i)\n",
    "    analyst_pred= pd.concat([analyst_pred, temp_df])\n",
    "\n",
    "analyst_pred1 = analyst_pred.assign(ERROR_RATE = analyst_pred['ERROR RATE'].str.replace('%', \"\"), \n",
    "                                    ACCURACY = analyst_pred['ACCURACY'].str.replace('%', \"\"))\n",
    "analyst_pred1.drop(['ERROR RATE', 'ACCURACY'], axis = 1, inplace= True)\n",
    "analyst_pred1.rename(columns = {\"url\":\"URL\"}, inplace=True)\n",
    "analyst_pred1.to_sql(\"analyst_stocks_covered\", engine, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2a5808",
   "metadata": {
    "id": "cc2a5808"
   },
   "source": [
    "- This table contains the following:\n",
    "        1. Ticker\n",
    "        2. Reports\n",
    "        3. Quarters\n",
    "        4. Points\n",
    "        5. PTS_EST\n",
    "        6. URL\n",
    "        7. Error Rate\n",
    "        \n",
    "- Due to the constant interruption of 403 Error, we decided to collect the lost URLS and run the code again, in order to retrieve all the information. The information is collected in lost_url list.\n",
    "\n",
    "- Scrolling the browser till the stocks covered table was required in order to fetch the information. Otherwise it would show \"loading\".\n",
    "\n",
    "\n",
    "- The table has **12745** records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_53capO-VHXw",
   "metadata": {
    "id": "_53capO-VHXw"
   },
   "source": [
    "### Pending Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c638a956",
   "metadata": {
    "id": "c638a956"
   },
   "outputs": [],
   "source": [
    "def pe_parallel_func(url_link):\n",
    "    c = webdriver.ChromeOptions()\n",
    "    c.add_argument(\"--incognito\")\n",
    "    \n",
    "    driver = webdriver.Chrome(options=c)\n",
    "    driver.implicitly_wait(50)\n",
    "    driver.get(url_link)\n",
    "    time.sleep(5)\n",
    "    driver.execute_script('window.scrollTo(0, 1200);')\n",
    "    time.sleep(5)    \n",
    "    try:\n",
    "        check = driver.find_elements(By.CLASS_NAME,'profile-section-message')[0].text\n",
    "        if len(check)==0:\n",
    "            num2 = int(driver.find_elements(By.CLASS_NAME,'profile-tbl-pagination-total-count')[1].text)      \n",
    "            if (num2 >5):\n",
    "                  for i in range(math.ceil(num2/20)):\n",
    "                        btn = driver.find_element(By.XPATH,'//*[@id=\"profile-pending-estimates\"]/div[2]')\n",
    "                        btn.click()\n",
    "                        time.sleep(3)\n",
    "\n",
    "            b = driver.find_elements(By.XPATH,'//*[@id=\"profile-pending-estimates\"]/div[1]/div[2]')[0].text\n",
    "            print(b)\n",
    "            b_new = re.sub(\"[0-9]+\\sdays ago\",\"\",b)\n",
    "            x_b = b_new.replace(\",\",\"\")\n",
    "            b1 = x_b.split(\"\\n\")\n",
    "            pending_estimates = pd.DataFrame(b1)\n",
    "            pending_estimates[\"URL\"] = url_link\n",
    "            path = \"C:/Users/Rida/Downloads/dbms/\"\n",
    "            pending_estimates = url_link.replace(\"https://www.estimize.com/users/\", \"\")\n",
    "            csv_path = path + 'PE_'+ analyst_name + \".csv\"\n",
    "            pending_estimates.to_csv(csv_path, index =False)\n",
    "        else:\n",
    "            print(\"No Pending estimates\")\n",
    "    except:\n",
    "        logging.warning(url_link)\n",
    "        \n",
    "            \n",
    "    driver.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532edd52",
   "metadata": {},
   "source": [
    "- Here 'check' is used to see which analyst has pending estimates. \n",
    "- If the analyst has no estimates we get the message \"No Pending estimates\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BhvNOSP_UfPp",
   "metadata": {
    "id": "BhvNOSP_UfPp"
   },
   "outputs": [],
   "source": [
    "analyst_ind_df = a_p.loc[a_p.Analyst_type != 'OVERALL', 'URL']\n",
    "analyst_ind_df1 = analyst_ind_df.drop_duplicates().reset_index(drop = True)\n",
    "url_links = analyst_ind_df1.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47Ppw_VbUW-S",
   "metadata": {
    "id": "47Ppw_VbUW-S"
   },
   "outputs": [],
   "source": [
    "for i in range(1,len(url_links)):\n",
    "    pe_parallel_func(url_links[i])\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28dbac9",
   "metadata": {},
   "source": [
    "- Here Parallel Function is used in order to fetch the data quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ApWXvU-bYYLZ",
   "metadata": {
    "id": "ApWXvU-bYYLZ"
   },
   "outputs": [],
   "source": [
    "path_3 = \"C:/Users/Rida/Downloads/dbms/\"\n",
    "files_2 = os.listdir(path_3)\n",
    "pending_estimate = pd.DataFrame()\n",
    "for i in range(len(files_2)):\n",
    "    print(i)\n",
    "    path_i = path_3 + files_2[i]\n",
    "    temp_df = pd.read_csv(path_i)\n",
    "    pending_estimate= pd.concat([pending_estimate, temp_df])\n",
    "\n",
    "\n",
    "pending_estimate = pending_estimate.reset_index(drop = True)\n",
    "pending_estimate['0'] = pending_estimate['0'].replace(\"\\s+\", \" \", regex = True)\n",
    "pending_estimate1 = pending_estimate['0'].str.split(' ', expand=True)\n",
    "pending_estimate1['URL'] = pending_estimate['URL']\n",
    "\n",
    "pending_estimate1['QUARTER'] = pending_estimate1[1] + \" \" + pending_estimate1[2]\n",
    "pending_estimate1['REPORTS'] = pending_estimate1[3] + \" \" + pending_estimate1[4] + \" \" + pending_estimate1[5]\n",
    "                               + \" \" + pending_estimate1[6]\n",
    "pending_estimate1['PUBLISHED'] = pending_estimate1[7] + \" \" + pending_estimate1[8] + \" \" + pending_estimate1[9]\n",
    "\n",
    "\n",
    "pending_estimate1.drop([1,2,3,4,5,6,7,8,9], axis = 1, inplace = True)\n",
    "pending_estimate1.rename(columns = {0:'TICKER', 10:'EPS', 11:'REVENUE'}, inplace = True)\n",
    "pending_estimate1.to_sql(\"analyst_pending_estimates\", engine, index = False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b734dec8",
   "metadata": {},
   "source": [
    "- The DataFrame Pending estimates includes the following information:\n",
    "       1. TICKER\n",
    "       2. EPS\n",
    "       3. Revenue\n",
    "       4. URL\n",
    "       5. Quarter\n",
    "       6. Reports\n",
    "       7. Published\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e682e2f",
   "metadata": {
    "id": "0e682e2f"
   },
   "source": [
    "- Not every analyst has Pending estimates. Hence this table has **2478** records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86374c55",
   "metadata": {
    "id": "86374c55"
   },
   "source": [
    "### Scored Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a6fffd",
   "metadata": {
    "id": "67a6fffd"
   },
   "outputs": [],
   "source": [
    "## Scored Estimtes\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25438088",
   "metadata": {
    "id": "25438088"
   },
   "outputs": [],
   "source": [
    "analysts_df = pd.read_csv(\"Analyst_URL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1935f01",
   "metadata": {
    "id": "a1935f01"
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "path = \"C:/Users/Rida/Downloads/table_3/\"\n",
    "lost_url = []\n",
    "for i in range(len(analysts_df)):\n",
    "    try:\n",
    "        url_i = analysts_df.iloc[i,0]\n",
    "        driver.get(url_i)\n",
    "        print(url_i)\n",
    "        driver.execute_script('window.scrollTo(0, 1200);')\n",
    "        time.sleep(10)\n",
    "\n",
    "        number = int(driver.find_elements_by_class_name('profile-tbl-pagination-total-count')[0].text)        \n",
    "        if(number>5):\n",
    "            for i in range(math.ceil(number/20)):\n",
    "                btn = driver.find_element_by_xpath('//*[@id=\"profile-scored-estimates\"]/div[2]')\n",
    "                btn.click()\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "        c = driver.find_elements_by_xpath('//*[@id=\"profile-scored-estimates\"]/div[1]/div[2]')[0].text\n",
    "        time.sleep(10)\n",
    "        li = list(c.split(\"\\n\"))\n",
    "        scored_estimates = pd.DataFrame(li)\n",
    "        scored_estimates[\"URL\"] = url_i\n",
    "        csv_path = path + 'S7_new'+ str(i) + \".csv\"\n",
    "        scored_estimates.to_csv(csv_path, index =False)\n",
    "        del scored_estimates\n",
    "    except:\n",
    "        lost_url.append(url_i) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pvyF4RqIXXhp",
   "metadata": {
    "id": "pvyF4RqIXXhp"
   },
   "outputs": [],
   "source": [
    "file_path_1 = 'C:/Users/Rida/Downloads/scored_estimates/'\n",
    "files_1 = os.listdir(file_path_1)\n",
    "scoring_estimate = pd.DataFrame()\n",
    "for i in range(len(files_1)):\n",
    "    print(i)\n",
    "    path_i = file_path_1 + files_1[i]\n",
    "    temp_df = pd.read_csv(path_i)\n",
    "    scoring_estimate= pd.concat([scoring_estimate, temp_df])\n",
    "scoring_estimate = scoring_estimate.reset_index(drop = True)\n",
    "scoring_estimate['0'] = scoring_estimate['0'].replace(\"- DEFUNCT \", \"\", regex = True)\n",
    "scoring_estimate1 = scoring_estimate['0'].str.split(' ', expand=True)\n",
    "scoring_estimate1['URL'] = scoring_estimate['URL']\n",
    "scoring_estimate1['QUARTER'] = scoring_estimate1[1] + ' ' + scoring_estimate1[2]\n",
    "scoring_estimate1['REPORTED'] = scoring_estimate1[3] + ' ' + scoring_estimate1[4] + ' ' + scoring_estimate1[5]\n",
    "scoring_estimate1['RANK'] = scoring_estimate1[6] + ' ' + scoring_estimate1[7] + ' ' + scoring_estimate1[8]\n",
    "scoring_estimate1.drop([1,2,3,4,5,6,7,8], axis =1, inplace = True)\n",
    "scoring_estimate1.rename(columns = {0:'TICKER', 9:'EPS_POINTS', 10:'REVENUE_POINTS', 11:'TOTAL_POINTS'}, inplace = True)\n",
    "scoring_estimate1.to_sql(\"analyst_scoring_estimate\", engine, index = False, if_exists = 'append')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dfeb1d",
   "metadata": {},
   "source": [
    "- Data Processing for Scored Estimate Table includes erasing extra test from the main columns and then bifurcating them into columns to obtain a proper table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e9346c",
   "metadata": {
    "id": "32e9346c"
   },
   "source": [
    "- This Scored Estimates table contains the following data:\n",
    "         1. Ticker\n",
    "         2. EPS_Points\n",
    "         3. Revenue_points\n",
    "         4. Total_Points\n",
    "         5. URL\n",
    "         6. Quarter\n",
    "         7. Reported\n",
    "         8. Rank\n",
    "         \n",
    "         \n",
    "         \n",
    "- This table has **113805** Records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a727c593",
   "metadata": {
    "id": "a727c593"
   },
   "source": [
    "### MYSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22c25ab",
   "metadata": {
    "id": "b22c25ab"
   },
   "source": [
    "After scraping the Data, the next task was to convert the Python Dataframes into My_SQL Tables, by using SQLAlchemy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7e5473",
   "metadata": {
    "id": "1b7e5473"
   },
   "source": [
    "**1) Given a ticker, how many analysts have made estimations for its EPS? Rank them by their confidence score, total points, error rate or accuracy percentile?**\n",
    "- Output: 2716 rows for AMZN. \n",
    "\n",
    "- Bill_Maurer has the highest score for Q1 2020.\n",
    "\n",
    "- Output is created for all the quarters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fdb60f",
   "metadata": {
    "id": "67fdb60f"
   },
   "source": [
    "**2) Given an industry, how many companies are covered, the average number of analysts, the average bias between the Estimize Consensus and the Reported Earnings?**\n",
    "\n",
    "\n",
    "- Output: 16 rows. \n",
    "\n",
    "- Software industry has the highest number of companies covered with average number of analysts of 608"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54c499a",
   "metadata": {
    "id": "a54c499a"
   },
   "source": [
    "**3) Which company have the largest number of analysts with confidence score greater than 7?**\n",
    "\n",
    "\n",
    "- Apple has the largest number of analysts with confidence score greater than 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29751fd4",
   "metadata": {
    "id": "29751fd4"
   },
   "source": [
    "**4) Who has the largest number of followers?**\n",
    "- Apple has largest number of followers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc012f96",
   "metadata": {
    "id": "fc012f96"
   },
   "source": [
    "### Regression Model (BONUS QUESTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e627ebd",
   "metadata": {
    "id": "3e627ebd"
   },
   "source": [
    "**Objective:** To identify the error rate of an analyst with respect to different scores of an analyst."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73baa44",
   "metadata": {
    "id": "a73baa44"
   },
   "source": [
    "Independent Variables:​\n",
    "\n",
    "1. Points​\n",
    "\n",
    "2. Accuracy Percentile​\n",
    "\n",
    "3. Points per estimate​\n",
    "\n",
    "4. Stocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af07c7",
   "metadata": {
    "id": "70af07c7"
   },
   "source": [
    "Target Variable:​\n",
    "\n",
    "1. Error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b6404e",
   "metadata": {
    "id": "67b6404e"
   },
   "outputs": [],
   "source": [
    "#Importing the files\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fb71e0",
   "metadata": {
    "id": "f2fb71e0"
   },
   "outputs": [],
   "source": [
    "user = 'root'\n",
    "password = 'root'\n",
    "host = 'localhost'\n",
    "port = 3306\n",
    "database = 'estimize'\n",
    "\n",
    "engine = create_engine(url=\"mysql+pymysql://{0}:{1}@{2}:{3}/{4}\".format(user, password, host, port, database))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797dad37",
   "metadata": {
    "id": "797dad37"
   },
   "outputs": [],
   "source": [
    "a= pd.read_sql(\"Select * from estimize.analyst_info\", engine)\n",
    "b = a.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b076ba3",
   "metadata": {
    "id": "6b076ba3"
   },
   "outputs": [],
   "source": [
    "b.ERROR_RATE = np.log((b[\"ERROR_RATE\"]))\n",
    "b.POINTS = np.sqrt(b[\"POINTS\"])\n",
    "b.ACCURACY_PERCENTILE =  np.sqrt((b[\"ACCURACY_PERCENTILE\"]))\n",
    "b.POINTS_PER_ESTIMATE = np.sqrt(b[\"POINTS_PER_ESTIMATE\"])\n",
    "b.POINTS_PER_ESTIMATE = np.sqrt(b[\"STOCKS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfa31cd",
   "metadata": {
    "id": "4bfa31cd"
   },
   "outputs": [],
   "source": [
    "b = b.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc5ab9b",
   "metadata": {
    "id": "2bc5ab9b"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "data1 = scaler.fit_transform(b[[\"ERROR_RATE\", \"POINTS\", \"ACCURACY_PERCENTILE\" ,\"POINTS_PER_ESTIMATE\", \"STOCKS\"]].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8aeed7",
   "metadata": {
    "id": "7e8aeed7"
   },
   "outputs": [],
   "source": [
    "new_data = pd.DataFrame(data1, columns = [\"ERROR_RATE\", \"POINTS\", \"ACCURACY_PERCENTILE\" ,\"POINTS_PER_ESTIMATE\", \"STOCKS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb62e10",
   "metadata": {
    "id": "5fb62e10"
   },
   "outputs": [],
   "source": [
    "new_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c7d2f4",
   "metadata": {
    "id": "85c7d2f4"
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn\")\n",
    "new_data.hist(figsize = (25,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c79d9c4",
   "metadata": {
    "id": "7c79d9c4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(16, 6))\n",
    "heatmap = sns.heatmap(new_data.corr(), vmin=-1, vmax=1, annot=True)\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b275bc8",
   "metadata": {
    "id": "4b275bc8"
   },
   "source": [
    "### Training and testing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b76999",
   "metadata": {
    "id": "e5b76999"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test,y_train, y_test = train_test_split(new_data.drop(\"ERROR_RATE\", axis=1),\n",
    "                                                  new_data[\"ERROR_RATE\"],\n",
    "                                                  test_size = 0.3,\n",
    "                                                  random_state = 0)\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e627be33",
   "metadata": {
    "id": "e627be33"
   },
   "source": [
    "### LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b284aede",
   "metadata": {
    "id": "b284aede"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error,mean_absolute_error\n",
    "linreg = LinearRegression()\n",
    "reg = linreg.fit(X_train, y_train)\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0e6db5",
   "metadata": {
    "id": "ad0e6db5"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "y_pred = linreg.predict(X_test)\n",
    "final_score = r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07b1b0d",
   "metadata": {
    "id": "b07b1b0d"
   },
   "outputs": [],
   "source": [
    "final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb57884",
   "metadata": {
    "id": "6eb57884"
   },
   "outputs": [],
   "source": [
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053a28f8",
   "metadata": {
    "id": "053a28f8"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e11e92a",
   "metadata": {
    "id": "9e11e92a"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "clf=RandomForestRegressor(n_estimators=100)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred=clf.predict(X_test)\n",
    "final_score = r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585a604e",
   "metadata": {
    "id": "585a604e"
   },
   "outputs": [],
   "source": [
    "final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a4e569",
   "metadata": {
    "id": "69a4e569"
   },
   "outputs": [],
   "source": [
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343bae5e",
   "metadata": {
    "id": "343bae5e"
   },
   "source": [
    "### LGBM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc5daed",
   "metadata": {
    "id": "ffc5daed"
   },
   "outputs": [],
   "source": [
    "import lightgbm as ltb\n",
    "ltb = ltb.LGBMRegressor()\n",
    "ltb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649e7833",
   "metadata": {
    "id": "649e7833"
   },
   "outputs": [],
   "source": [
    "y_pred=ltb.predict(X_test)\n",
    "final_score = r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50518543",
   "metadata": {
    "id": "50518543"
   },
   "outputs": [],
   "source": [
    "final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2299785",
   "metadata": {
    "id": "a2299785"
   },
   "outputs": [],
   "source": [
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0b5573",
   "metadata": {
    "id": "ad0b5573"
   },
   "source": [
    "### XGB Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acfa0a8",
   "metadata": {
    "id": "2acfa0a8"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xgboost = XGBRegressor(n_estimators=100, max_depth=10, eta=0.1, subsample=0.7, colsample_bytree=0.8)\n",
    "xgboost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8303ee5c",
   "metadata": {
    "id": "8303ee5c"
   },
   "outputs": [],
   "source": [
    "y_pred=xgboost.predict(X_test)\n",
    "final_score = r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d637bc",
   "metadata": {
    "id": "c8d637bc"
   },
   "outputs": [],
   "source": [
    "final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06563c2b",
   "metadata": {
    "id": "06563c2b"
   },
   "outputs": [],
   "source": [
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "MSE"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "74b60c62",
    "61f04d49",
    "a727c593",
    "fc012f96",
    "4b275bc8",
    "e627be33",
    "053a28f8",
    "343bae5e",
    "ad0b5573"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
